{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# notebook source: \n",
    "# https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter04_convolutional-neural-networks/cnn-gluon.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "kernel_size = (5,5)\n",
    "pool_size = (2,2)\n",
    "filters_layer1 = 32\n",
    "filters_layer2 = 64\n",
    "units_dense = 1024\n",
    "# activation='relu'\n",
    "\n",
    "# train parameters\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "dropout = 0.5\n",
    "# loss: softmax\n",
    "# optimizer: adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in ``gluon``\n",
    "\n",
    "Now let's see how succinctly we can express a convolutional neural network using ``gluon``. You might be relieved to find out that this too requires hardly any more code than a logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cpu(0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to change CPU to GPU set mx.gpu instead of mx.cpu\n",
    "ctx = mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monic\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:118: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "C:\\Users\\monic\\Anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision.py:122: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "#num_inputs = 784\n",
    "#num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a convolutional neural network\n",
    "\n",
    "Again, a few lines here is all we need in order to change the model. Let's add a couple of convolutional layers using ``gluon.nn``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same network as keras demo\n",
    "num_outputs=10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=filters_layer1, kernel_size=kernel_size[0], activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=pool_size[0])) # , strides=2)\n",
    "    net.add(gluon.nn.Conv2D(channels=filters_layer2, kernel_size=kernel_size[0], activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=pool_size[0]))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(units_dense, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dropout(dropout))\n",
    "    net.add(gluon.nn.Dense(num_outputs)) #softmax activation is automatically derived by the loss function used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu(0)\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'Adam')#, {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write evaluation loop to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.772982120513916\n",
      "seconds per epoch\n",
      "55.472376346588135\n",
      "seconds per epoch\n",
      "55.119585037231445\n",
      "seconds per epoch\n",
      "54.95815587043762\n",
      "seconds per epoch\n",
      "55.217846155166626\n",
      "seconds per epoch\n",
      "58.89562726020813\n",
      "seconds per epoch\n",
      "62.48016691207886\n",
      "seconds per epoch\n",
      "64.74417519569397\n",
      "seconds per epoch\n",
      "57.583136796951294\n",
      "seconds per epoch\n",
      "59.71279954910278\n",
      "seconds per epoch\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    start = time.time()\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # commented because they slow down the training\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        #curr_loss = nd.mean(loss).asscalar()\n",
    "        #moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "        #               else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "        \n",
    "    #test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    #train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    #print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))  \n",
    "    print(time.time()-start)\n",
    "    print('seconds per epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 1, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_acc 0.9981333333333333, Test_acc 0.993\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = evaluate_accuracy(test_data, net)\n",
    "train_accuracy = evaluate_accuracy(train_data, net)\n",
    "print(\"Train_acc %s, Test_acc %s\" % (train_accuracy, test_accuracy))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions per second: \n",
      "63814.4433614644\n",
      "Number of predictions per second: \n",
      "63746.24934694847\n",
      "Number of predictions per second: \n",
      "63844.79866809371\n",
      "Number of predictions per second: \n",
      "127613.71808889945\n",
      "Number of predictions per second: \n",
      "63791.695817490494\n",
      "Number of predictions per second: \n",
      "127583.39163498099\n",
      "Number of predictions per second: \n",
      "127644.05896338564\n",
      "Number of predictions per second: \n",
      "128438.01722488039\n",
      "Number of predictions per second: \n",
      "63761.390973871734\n",
      "Number of predictions per second: \n",
      "127553.07959135187\n",
      "Number of predictions per second: \n",
      "127462.22981956316\n"
     ]
    }
   ],
   "source": [
    "# measure deploy speed\n",
    "for i, (data, label) in enumerate(test_data):\n",
    "    if i>10:\n",
    "        break\n",
    "    start = time.time()\n",
    "    data = data.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    output = net(data)\n",
    "    predictions = nd.argmax(output, axis=1)\n",
    "    duration = time.time() - start\n",
    "    print('Number of predictions per second: ')\n",
    "    print(np.shape(data)[0]/duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 15.094281  -11.709779   -9.53958    -7.927335   -6.6310296  -3.9149308\n",
      "   -3.0588708  -9.9343815  -0.4707301  -1.846066 ]]\n",
      "<NDArray 1x10 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 15.094281  -11.709779   -9.53958    -7.927335   -6.6310296  -3.9149308\n",
       "   -3.0588708  -9.9343815  -0.4707301  -1.846066 ]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save and load model (! only weights), check same prediction\n",
    "# source: http://gluon.mxnet.io/chapter03_deep-neural-networks/serialization.html\n",
    "filename = \"testnet.params\"\n",
    "net.save_params(filename)\n",
    "print(net(data[0:1]))\n",
    "\n",
    "net2 = gluon.nn.Sequential()\n",
    "with net2.name_scope():\n",
    "    net2.add(gluon.nn.Conv2D(channels=filters_layer1, kernel_size=kernel_size[0], activation='relu'))\n",
    "    net2.add(gluon.nn.MaxPool2D(pool_size=pool_size[0]))\n",
    "    net2.add(gluon.nn.Conv2D(channels=filters_layer2, kernel_size=kernel_size[0], activation='relu'))\n",
    "    net2.add(gluon.nn.MaxPool2D(pool_size=pool_size[0]))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net2.add(gluon.nn.Flatten())\n",
    "    net2.add(gluon.nn.Dense(units_dense, activation=\"relu\"))\n",
    "    net2.add(gluon.nn.Dropout(dropout))\n",
    "    net2.add(gluon.nn.Dense(num_outputs)) #softmax activation is automatically derived by the loss function used\n",
    "\n",
    "net2.load_params(filename, ctx=ctx)\n",
    "net2(data[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You might notice that by using ``gluon``, we get code that runs much faster whether on CPU or GPU. That's largely because ``gluon`` can call down to highly optimized layers that have been written in C++. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
