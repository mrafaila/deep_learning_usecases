{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading...\n",
      "Dataset already downloaded. Did not download twice.\n",
      "\n",
      "Extracting...\n",
      "Dataset already extracted. Did not extract twice.\n",
      "\n",
      "\n",
      "Dataset is now located at: ../data/UCI HAR Dataset/\n",
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n",
      "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "\n",
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] \n",
    "\n",
    "\n",
    "# ## Let's start by downloading the data: \n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import os\n",
    "# Note: Linux bash commands start with a \"!\" inside those \"ipython notebook\" cells\n",
    "\n",
    "DATA_PATH = \"./\"\n",
    "get_ipython().system('python download_dataset.py')\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n",
    "\n",
    "\n",
    "# ## Preparing dataset:\n",
    "\n",
    "\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "\n",
    "# ## Additionnal Parameters:\n",
    "# \n",
    "# Here are some core parameter definitions for the training. \n",
    "# \n",
    "# The whole neural network's structure could be summarised by enumerating those parameters and the fact an LSTM is used. \n",
    "# dropout=0.2, recurrent_dropout=0.2\n",
    "# nb_epoch=5, batch_size=64\n",
    "# \n",
    "# Keras LSTM with dropout is used.\n",
    "# A convolutional layer and max pooling can also be tested, before the LSTM cell.\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [4],\n",
       "       [4],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process batches of 10 days\n",
    "BATCH_SIZE =64\n",
    "X=dict()\n",
    "y=dict()\n",
    "X['train']=X_train\n",
    "y['train']=y_train\n",
    "X['test']=X_test\n",
    "y['test']=y_test\n",
    "def next_batch(x, y, ds):\n",
    "    \"\"\"get the next batch for training\"\"\"\n",
    "\n",
    "    def as_batch(data, start, count):\n",
    "        return data[start:start + count]\n",
    "\n",
    "    for i in range(0, len(x[ds]), BATCH_SIZE):\n",
    "        yield as_batch(X[ds], i, BATCH_SIZE), as_batch(y[ds], i, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monic\\Anaconda3\\lib\\site-packages\\cntk\\core.py:82: RuntimeWarning: data is not C contiguous; rearrange your data/computation to avoid costly data conversions\n",
      "  RuntimeWarning)\n",
      "C:\\Users\\monic\\Anaconda3\\lib\\site-packages\\cntk\\core.py:350: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  elif np.issubdtype(sample.dtype, int):\n",
      "C:\\Users\\monic\\Anaconda3\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"int32\", but your input variable (uid \"Input28350\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Values for 1 required arguments 'Input('Input27565', [#, *], [9])', that the requested output(s) 'Output('aggregateLoss', [], []), Output('Block28362_Output_0', [#], [1]), Output('aggregateEvalMetric', [], [])' depend on, have not been provided.\n\n[CALL STACK]\n    > CNTK::Internal::  UseSparseGradientAggregationInDataParallelSGD\n    - CNTK::Function::  Forward\n    - CNTK::  CreateTrainer\n    - CNTK::Trainer::  TotalNumberOfUnitsSeen\n    - CNTK::Trainer::  TrainMinibatch (x2)\n    - PyInit__cntk_py (x2)\n    - PyEval_EvalFrameDefault\n    - Py_CheckFunctionResult\n    - PyObject_CallFunctionObjArgs\n    - PyEval_EvalFrameDefault\n    - Py_CheckFunctionResult\n    - PyObject_CallFunctionObjArgs\n    - PyEval_EvalFrameDefault\n    - Py_CheckFunctionResult\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-06332d3170e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ml_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cntk\\train\\trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments, is_sweep_end,\n\u001b[1;32m--> 184\u001b[1;33m                     device)\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cntk\\cntk_py.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   2854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2855\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2856\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2858\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Values for 1 required arguments 'Input('Input27565', [#, *], [9])', that the requested output(s) 'Output('aggregateLoss', [], []), Output('Block28362_Output_0', [#], [1]), Output('aggregateEvalMetric', [], [])' depend on, have not been provided.\n\n[CALL STACK]\n    > CNTK::Internal::  UseSparseGradientAggregationInDataParallelSGD\n    - CNTK::Function::  Forward\n    - CNTK::  CreateTrainer\n    - CNTK::Trainer::  TotalNumberOfUnitsSeen\n    - CNTK::Trainer::  TrainMinibatch (x2)\n    - PyInit__cntk_py (x2)\n    - PyEval_EvalFrameDefault\n    - Py_CheckFunctionResult\n    - PyObject_CallFunctionObjArgs\n    - PyEval_EvalFrameDefault\n    - Py_CheckFunctionResult\n    - PyObject_CallFunctionObjArgs\n    - PyEval_EvalFrameDefault\n    - Py_CheckFunctionResult\n\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "EPOCHS =10\n",
    "#Specify the internal-state dimensions of the LSTM cell\n",
    "H_DIMS = 128\n",
    "def create_model(features, num_classes, H_DIMS):\n",
    "    \"\"\"Create the model for time series prediction\"\"\"\n",
    "    with C.layers.default_options(initial_state = 0.1):\n",
    "        m = C.layers.Recurrence(C.layers.LSTM(H_DIMS))(features)\n",
    "        m = C.sequence.last(m)\n",
    "        m = C.layers.Dropout(0.2)(m)\n",
    "        m = C.layers.Dense(num_classes)(m)\n",
    "        return m\n",
    "# Input variables denoting the features and label data\n",
    "features = C.sequence.input_variable(shape=input_dim, is_sparse=True)\n",
    "\n",
    "# Instantiate the sequence classification model\n",
    "z = create_model(features, num_classes, H_DIMS)\n",
    "\n",
    "# input sequences\n",
    "features = C.sequence.input_variable(9)\n",
    "\n",
    "\n",
    "# expected output (label), also the dynamic axes of the model output\n",
    "# is specified as the model of the label input\n",
    "label = C.input_variable(num_classes, dynamic_axes=z.dynamic_axes, name=\"y\")\n",
    "\n",
    "# the learning rate\n",
    "learning_rate = 0.005\n",
    "lr_schedule = C.learning_parameter_schedule(learning_rate)\n",
    "\n",
    "# loss function\n",
    "ce = C.cross_entropy_with_softmax(z, label)\n",
    "pe = C.classification_error(z, label)\n",
    "\n",
    "\n",
    "# use adam optimizer\n",
    "momentum_schedule = C.momentum_schedule(0.9, minibatch_size=BATCH_SIZE)\n",
    "learner = C.fsadagrad(z.parameters, \n",
    "                      lr = lr_schedule, \n",
    "                      momentum = momentum_schedule)\n",
    "trainer = C.Trainer(z, (ce, pe), [learner])\n",
    "# training\n",
    "loss_summary = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(0, EPOCHS):\n",
    "    for x_batch, l_batch in next_batch(X, y, \"train\"):\n",
    "        trainer.train_minibatch({features: x_batch, label: l_batch})\n",
    "        \n",
    "    if epoch % (EPOCHS / 10) == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        loss_summary.append(training_loss)\n",
    "        print(\"epoch: {}, loss: {:.4f}\".format(epoch, training_loss))\n",
    "\n",
    "print(\"Training took {:.1f} sec\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monic\\Anaconda3\\lib\\site-packages\\cntk\\core.py:82: RuntimeWarning: data is not C contiguous; rearrange your data/computation to avoid costly data conversions\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "variable with name \"features\" does not exist in the network. Available variable names: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9b62675f4eda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m251\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mmb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mevaluation_average\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_minibatch_evaluation_average\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cntk\\train\\trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mall_args\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             arguments = sanitize_var_map(tuple(all_args), arguments,\n\u001b[1;32m--> 149\u001b[1;33m                 extract_values_from_minibatch_data = False, device=device)\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mcontains_minibatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\cntk\\internal\\sanitize.py\u001b[0m in \u001b[0;36msanitize_var_map\u001b[1;34m(op_arguments, arguments, precision, device, extract_values_from_minibatch_data)\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 raise ValueError('variable with name \"%s\" does not exist in the network. Available variable names: %s' % (\n\u001b[1;32m--> 393\u001b[1;33m                     var, \", \".join(var_name_map)))\n\u001b[0m\u001b[0;32m    394\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mname_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'node name \"%s\" is not unique'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: variable with name \"features\" does not exist in the network. Available variable names: "
     ]
    }
   ],
   "source": [
    "import cntk as C\n",
    "    \n",
    "\n",
    "# Defines the LSTM model for classifying sequences\n",
    "def lstm_sequence_classifier(features, num_classes, LSTM_dim):\n",
    "    classifier = C.layers.Sequential([\n",
    "                                      C.layers.Recurrence(C.layers.LSTM(LSTM_dim)),\n",
    "                                      C.sequence.last,\n",
    "                                      C.layers.Dense(num_classes)])\n",
    "    return classifier(features)\n",
    "\n",
    "\n",
    "# Creates and trains a LSTM sequence classification model\n",
    "#def train_sequence_classifier():\n",
    "input_dim = 9\n",
    "hidden_dim = 128\n",
    "num_classes = 6\n",
    "\n",
    "# Input variables denoting the features and label data\n",
    "features = C.sequence.input_variable(shape=input_dim, is_sparse=True)\n",
    "label = C.input_variable(num_classes)\n",
    "\n",
    "# Instantiate the sequence classification model\n",
    "classifier_output = lstm_sequence_classifier(features, num_classes, hidden_dim)\n",
    "\n",
    "ce = C.cross_entropy_with_softmax(classifier_output, label)\n",
    "pe = C.classification_error(classifier_output, label)\n",
    "\n",
    "train_reader = C.io.MinibatchSourceFromData(dict(x=np.array(X_train, np.float32), y=np.array(y_train, np.float32)))\n",
    "#reader = create_reader(path, True, input_dim, num_classes)\n",
    "\n",
    "\n",
    "lr_per_sample = C.learning_parameter_schedule_per_sample(0.1)\n",
    "\n",
    "# Instantiate the trainer object to drive the model training\n",
    "progress_printer = C.logging.ProgressPrinter(0)\n",
    "trainer = C.Trainer(classifier_output, (ce, pe),\n",
    "                    C.sgd(classifier_output.parameters, lr=lr_per_sample),\n",
    "                    progress_printer)\n",
    "\n",
    "# Get minibatches of sequences to train with and perform model training\n",
    "minibatch_size = 200\n",
    "\n",
    "for i in range(251):\n",
    "    mb = train_reader.next_minibatch(minibatch_size)\n",
    "    trainer.train_minibatch(dict(features = mb[train_reader.streams['x']],label = mb[train_reader.streams['y']]))\n",
    "\n",
    "evaluation_average = copy.copy(train_reader.previous_minibatch_evaluation_average)\n",
    "loss_average = copy.copy(train_reader.previous_minibatch_loss_average)\n",
    "\n",
    "#error, _ = train_sequence_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cntk as C\n",
    "import copy\n",
    "\n",
    "\n",
    "# Creates the reader\n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        features = C.io.StreamDef(field='x', shape=input_dim,   is_sparse=True),\n",
    "        labels   = C.io.StreamDef(field='y', shape=label_dim,   is_sparse=False)\n",
    "    )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "\n",
    "# Defines the LSTM model for classifying sequences\n",
    "def lstm_sequence_classifier(features, num_classes, embedding_dim, LSTM_dim):\n",
    "    classifier = C.layers.Sequential([C.layers.Embedding(embedding_dim),\n",
    "                                      C.layers.Recurrence(C.layers.LSTM(LSTM_dim)),\n",
    "                                      C.sequence.last,\n",
    "                                      C.layers.Dense(num_classes)])\n",
    "    return classifier(features)\n",
    "\n",
    "\n",
    "# Creates and trains a LSTM sequence classification model\n",
    "def train_sequence_classifier():\n",
    "    input_dim = 2000\n",
    "    hidden_dim = 25\n",
    "    embedding_dim = 50\n",
    "    num_classes = 5\n",
    "\n",
    "    # Input variables denoting the features and label data\n",
    "    features = C.sequence.input_variable(shape=input_dim, is_sparse=True)\n",
    "    label = C.input_variable(num_classes)\n",
    "\n",
    "    # Instantiate the sequence classification model\n",
    "    classifier_output = lstm_sequence_classifier(features, num_classes, embedding_dim, hidden_dim)\n",
    "\n",
    "    ce = C.cross_entropy_with_softmax(classifier_output, label)\n",
    "    pe = C.classification_error(classifier_output, label)\n",
    "\n",
    "    \n",
    "\n",
    "    reader = create_reader(path, True, input_dim, num_classes)\n",
    "\n",
    "    input_map = {\n",
    "        features : reader.streams.features,\n",
    "        label    : reader.streams.labels\n",
    "    }\n",
    "\n",
    "    lr_per_sample = C.learning_parameter_schedule_per_sample(0.1)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    progress_printer = C.logging.ProgressPrinter(0)\n",
    "    trainer = C.Trainer(classifier_output, (ce, pe),\n",
    "                        C.sgd(classifier_output.parameters, lr=lr_per_sample),\n",
    "                        progress_printer)\n",
    "\n",
    "    # Get minibatches of sequences to train with and perform model training\n",
    "    minibatch_size = 200\n",
    "\n",
    "    for i in range(251):\n",
    "        mb = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "        trainer.train_minibatch(mb)\n",
    "\n",
    "    evaluation_average = copy.copy(trainer.previous_minibatch_evaluation_average)\n",
    "    loss_average = copy.copy(trainer.previous_minibatch_loss_average)\n",
    "\n",
    "    return evaluation_average, loss_average\n",
    "\n",
    "    error, _ = train_sequence_classifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
